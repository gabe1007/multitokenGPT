{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T22:22:12.469155Z","iopub.status.busy":"2024-09-23T22:22:12.468851Z","iopub.status.idle":"2024-09-23T22:22:16.757293Z","shell.execute_reply":"2024-09-23T22:22:16.756093Z","shell.execute_reply.started":"2024-09-23T22:22:12.469126Z"},"trusted":true},"outputs":[],"source":["# Common imports \n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T22:22:31.086013Z","iopub.status.busy":"2024-09-23T22:22:31.085632Z","iopub.status.idle":"2024-09-23T22:22:31.127161Z","shell.execute_reply":"2024-09-23T22:22:31.125868Z","shell.execute_reply.started":"2024-09-23T22:22:31.085984Z"},"trusted":true},"outputs":[],"source":["BATCH_SIZE = 128\n","BLOCK_SIZE = 256 \n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","N_EMBED = 256\n","N_HEADS = 4\n","DROPOUT = 0.2\n","N_LAYER = 5\n","LEARNING_RATE = 3e-3\n","MAX_ITERS = 10000\n","EVAL_INTERVAL = 500\n","EVAL_ITERS = 200"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T22:22:31.691593Z","iopub.status.busy":"2024-09-23T22:22:31.690888Z","iopub.status.idle":"2024-09-23T22:22:33.082266Z","shell.execute_reply":"2024-09-23T22:22:33.081158Z","shell.execute_reply.started":"2024-09-23T22:22:31.691558Z"},"trusted":true},"outputs":[],"source":["# This notebook was last run in a Kaggle environment, so you must replace it\n","with open('/kaggle/input/40k_text_dataset/pytorch/default/1/40k.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","# Unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","VOCAB_SIZE = len(chars)\n","\n","# Map the characters\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s]\n","decode = lambda l: ''.join([itos[i] for i in l]) \n","\n","# Get data splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) \n","train = data[:n]\n","val = data[n:]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T22:22:33.084596Z","iopub.status.busy":"2024-09-23T22:22:33.084228Z","iopub.status.idle":"2024-09-23T22:22:33.092150Z","shell.execute_reply":"2024-09-23T22:22:33.091091Z","shell.execute_reply.started":"2024-09-23T22:22:33.084549Z"},"trusted":true},"outputs":[],"source":["def get_batch(split, n_pred_tokens=4): # n_tokens: number of future tokens to predict\n","    # generate a small batch of data of inputs x and targets y\n","    data = train if split == 'train' else val\n","    ix = torch.randint(len(data) - BLOCK_SIZE - n_pred_tokens + 1, (BATCH_SIZE,))  # Adjust for n_tokens\n","    x = torch.stack([data[i:i + BLOCK_SIZE] for i in ix])\n","    y = torch.stack([data[i+1:i + BLOCK_SIZE + n_pred_tokens] for i in ix]) # Get multiple future tokens\n","    x, y = x.to(DEVICE), y.to(DEVICE)\n","    return x, y"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T22:22:33.093805Z","iopub.status.busy":"2024-09-23T22:22:33.093439Z","iopub.status.idle":"2024-09-23T22:22:33.101902Z","shell.execute_reply":"2024-09-23T22:22:33.100815Z","shell.execute_reply.started":"2024-09-23T22:22:33.093776Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(EVAL_ITERS)\n","        for k in range(EVAL_ITERS):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T22:22:33.372171Z","iopub.status.busy":"2024-09-23T22:22:33.371264Z","iopub.status.idle":"2024-09-23T22:22:33.392819Z","shell.execute_reply":"2024-09-23T22:22:33.391425Z","shell.execute_reply.started":"2024-09-23T22:22:33.372136Z"},"trusted":true},"outputs":[],"source":["class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(N_EMBED, head_size, bias=False) # head_size x N_EMBED\n","        self.query = nn.Linear(N_EMBED, head_size, bias=False)\n","        self.value = nn.Linear(N_EMBED, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n","\n","        self.dropout = nn.Dropout(DROPOUT)\n","\n","    def forward(self, x):\n","        # input of size (B, T, C)\n","        # output of size (B, T, head_size)\n","        B,T,C = x.shape\n","        #  x = B,T,C = 16, 256, 512,  self.key = 64 x 512, linear performs xAT+b,\n","        # since nn.Linear inverts the order we have to transpose self.key to get 16 x 256 x 512 @ 512 x 64\n","        k = self.key(x)\n","        q = self.query(x) # (B,T,hs)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T) dim=-1 means that the softmax function is applied along the last dimension of the tensor.\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,hs)\n","        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, N_EMBED) # 512 x 512\n","        self.dropout = nn.Dropout(DROPOUT)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(DROPOUT),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T22:39:36.024685Z","iopub.status.busy":"2024-09-23T22:39:36.023813Z","iopub.status.idle":"2024-09-23T22:39:36.043108Z","shell.execute_reply":"2024-09-23T22:39:36.042037Z","shell.execute_reply.started":"2024-09-23T22:39:36.024649Z"},"trusted":true},"outputs":[],"source":["class GPTLanguageModel(nn.Module):\n","    def __init__(self, num_pred_tokens=4):  \n","        super().__init__()\n","        self.num_pred_tokens = num_pred_tokens\n","        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, N_EMBED)\n","        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBED)\n","        self.blocks = nn.Sequential(*[Block(N_EMBED, n_head=N_HEADS) for _ in range(N_LAYER)])\n","        self.ln_f = nn.LayerNorm(N_EMBED)\n","        self.lm_heads = nn.ModuleList([nn.Linear(N_EMBED, VOCAB_SIZE) for _ in range(num_pred_tokens)])\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","        # Token and position embeddings\n","        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))  # (T, C)\n","        x = tok_emb + pos_emb  # (B, T, C)\n","        x = self.blocks(x)  # (B, T, C)\n","        x = self.ln_f(x)  # (B, T, C)\n","        \n","        # Apply each head\n","        logits = [head(x) for head in self.lm_heads]\n","        logits = torch.stack(logits, dim=2)  # Stack over the num_pred_tokens dimension: (B, T, num_pred_tokens, vocab_size)\n","        \n","        if targets is None:\n","            loss = None\n","        else: \n","            B, T, N, C = logits.shape\n","            losses = []\n","            for i in range(N):\n","                head_logits = logits[:, :, i, :].reshape(B * T, C)\n","                head_targets = targets[:, i:i + T].reshape(B * T) \n","                losses.append(F.cross_entropy(head_logits, head_targets))\n","            loss = torch.mean(torch.stack(losses))\n","        return logits, loss\n","    \n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -BLOCK_SIZE:]\n","\n","            # get the predictions (logits from the first head only)\n","            logits, _ = self(idx_cond)  # logits: (B, T, num_pred_tokens, vocab_size)\n","\n","            # Use only the first head for token generation\n","            logits_head = logits[:, -1, , :]  # (B, vocab_size)\n","\n","            # Apply softmax to get probabilities for the next token\n","            probs = F.softmax(logits_head, dim=-1)  # (B, vocab_size)\n","\n","            # Sample from the distribution for the next token\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n","\n","            # Append the sampled token to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1)  # (B, T + 1)\n","\n","        return idx"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T22:22:35.495451Z","iopub.status.busy":"2024-09-23T22:22:35.494481Z","iopub.status.idle":"2024-09-23T22:22:35.502066Z","shell.execute_reply":"2024-09-23T22:22:35.500954Z","shell.execute_reply.started":"2024-09-23T22:22:35.495417Z"},"trusted":true},"outputs":[],"source":["# Save the model checkpoint\n","def save_checkpoint(model, optimizer, filename=\"model.pth\"):\n","    checkpoint = {\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n","    \n","    \n","def load_checkpoint(model, optimizer, filename=\"model.pth\"):\n","    checkpoint = torch.load(filename)\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","    return model, optimizer"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2024-09-21T23:13:53.006399Z","iopub.status.busy":"2024-09-21T23:13:53.005808Z","iopub.status.idle":"2024-09-22T01:22:45.272445Z","shell.execute_reply":"2024-09-22T01:22:45.271367Z","shell.execute_reply.started":"2024-09-21T23:13:53.006366Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["4.131704 M parameters\n","step 0: train loss 4.5979, val loss 4.5989\n","step 500: train loss 2.6752, val loss 2.7004\n","step 1000: train loss 2.3533, val loss 2.3999\n","step 1500: train loss 2.1757, val loss 2.2367\n","step 2000: train loss 2.0959, val loss 2.1594\n","step 2500: train loss 2.0519, val loss 2.1288\n","step 3000: train loss 2.0194, val loss 2.0969\n","step 3500: train loss 1.9993, val loss 2.0843\n","step 4000: train loss 1.9808, val loss 2.0644\n","step 4500: train loss 1.9678, val loss 2.0588\n","step 5000: train loss 1.9527, val loss 2.0475\n","step 5500: train loss 1.9457, val loss 2.0369\n","step 6000: train loss 1.9378, val loss 2.0340\n","step 6500: train loss 1.9282, val loss 2.0293\n","step 7000: train loss 1.9215, val loss 2.0268\n","step 7500: train loss 1.9168, val loss 2.0227\n","step 8000: train loss 1.9083, val loss 2.0175\n","step 8500: train loss 1.9085, val loss 2.0212\n","step 9000: train loss 1.8992, val loss 2.0119\n","step 9500: train loss 1.8955, val loss 2.0107\n","step 10000: train loss 1.8915, val loss 2.0072\n","step 10500: train loss 1.8872, val loss 2.0068\n","step 11000: train loss 1.8840, val loss 2.0043\n","step 11500: train loss 1.8784, val loss 1.9997\n","step 12000: train loss 1.8771, val loss 2.0017\n","step 12500: train loss 1.8712, val loss 1.9952\n","step 13000: train loss 1.8694, val loss 1.9987\n","step 13500: train loss 1.8711, val loss 1.9966\n","step 14000: train loss 1.8657, val loss 1.9954\n","step 14500: train loss 1.8622, val loss 1.9917\n","step 15000: train loss 1.8624, val loss 1.9909\n","step 15500: train loss 1.8575, val loss 1.9931\n","step 16000: train loss 1.8553, val loss 1.9944\n","step 16500: train loss 1.8556, val loss 1.9894\n","step 17000: train loss 1.8527, val loss 1.9929\n","step 17500: train loss 1.8493, val loss 1.9843\n","step 18000: train loss 1.8463, val loss 1.9876\n","step 18500: train loss 1.8444, val loss 1.9798\n","step 19000: train loss 1.8414, val loss 1.9863\n","step 19500: train loss 1.8414, val loss 1.9845\n","step 20000: train loss 1.8408, val loss 1.9817\n","step 20500: train loss 1.8411, val loss 1.9852\n","step 21000: train loss 1.8358, val loss 1.9766\n","step 21500: train loss 1.8374, val loss 1.9790\n","step 22000: train loss 1.8331, val loss 1.9788\n","step 22500: train loss 1.8334, val loss 1.9821\n","step 23000: train loss 1.8298, val loss 1.9771\n","step 23500: train loss 1.8282, val loss 1.9761\n","step 24000: train loss 1.8286, val loss 1.9748\n","step 24500: train loss 1.8288, val loss 1.9855\n","step 24999: train loss 1.8292, val loss 1.9830\n"]}],"source":["model = GPTLanguageModel()\n","m = model.to(DEVICE)\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE)\n","\n","for iter in range(MAX_ITERS):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % EVAL_INTERVAL == 0 or iter == MAX_ITERS - 1:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save the model after the training loop\n","save_checkpoint(model, optimizer, filename=\"multitoken_40k_50000.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T22:39:42.023170Z","iopub.status.busy":"2024-09-23T22:39:42.022314Z","iopub.status.idle":"2024-09-23T22:39:42.317108Z","shell.execute_reply":"2024-09-23T22:39:42.316221Z","shell.execute_reply.started":"2024-09-23T22:39:42.023136Z"},"trusted":true},"outputs":[{"data":{"text/plain":["GPTLanguageModel(\n","  (token_embedding_table): Embedding(94, 256)\n","  (position_embedding_table): Embedding(256, 256)\n","  (blocks): Sequential(\n","    (0): Block(\n","      (sa): MultiHeadAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x Head(\n","            (key): Linear(in_features=256, out_features=64, bias=False)\n","            (query): Linear(in_features=256, out_features=64, bias=False)\n","            (value): Linear(in_features=256, out_features=64, bias=False)\n","            (dropout): Dropout(p=0.2, inplace=False)\n","          )\n","        )\n","        (proj): Linear(in_features=256, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","      )\n","      (ffwd): FeedFoward(\n","        (net): Sequential(\n","          (0): Linear(in_features=256, out_features=1024, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=1024, out_features=256, bias=True)\n","          (3): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (1): Block(\n","      (sa): MultiHeadAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x Head(\n","            (key): Linear(in_features=256, out_features=64, bias=False)\n","            (query): Linear(in_features=256, out_features=64, bias=False)\n","            (value): Linear(in_features=256, out_features=64, bias=False)\n","            (dropout): Dropout(p=0.2, inplace=False)\n","          )\n","        )\n","        (proj): Linear(in_features=256, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","      )\n","      (ffwd): FeedFoward(\n","        (net): Sequential(\n","          (0): Linear(in_features=256, out_features=1024, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=1024, out_features=256, bias=True)\n","          (3): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (2): Block(\n","      (sa): MultiHeadAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x Head(\n","            (key): Linear(in_features=256, out_features=64, bias=False)\n","            (query): Linear(in_features=256, out_features=64, bias=False)\n","            (value): Linear(in_features=256, out_features=64, bias=False)\n","            (dropout): Dropout(p=0.2, inplace=False)\n","          )\n","        )\n","        (proj): Linear(in_features=256, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","      )\n","      (ffwd): FeedFoward(\n","        (net): Sequential(\n","          (0): Linear(in_features=256, out_features=1024, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=1024, out_features=256, bias=True)\n","          (3): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (3): Block(\n","      (sa): MultiHeadAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x Head(\n","            (key): Linear(in_features=256, out_features=64, bias=False)\n","            (query): Linear(in_features=256, out_features=64, bias=False)\n","            (value): Linear(in_features=256, out_features=64, bias=False)\n","            (dropout): Dropout(p=0.2, inplace=False)\n","          )\n","        )\n","        (proj): Linear(in_features=256, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","      )\n","      (ffwd): FeedFoward(\n","        (net): Sequential(\n","          (0): Linear(in_features=256, out_features=1024, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=1024, out_features=256, bias=True)\n","          (3): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (4): Block(\n","      (sa): MultiHeadAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x Head(\n","            (key): Linear(in_features=256, out_features=64, bias=False)\n","            (query): Linear(in_features=256, out_features=64, bias=False)\n","            (value): Linear(in_features=256, out_features=64, bias=False)\n","            (dropout): Dropout(p=0.2, inplace=False)\n","          )\n","        )\n","        (proj): Linear(in_features=256, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","      )\n","      (ffwd): FeedFoward(\n","        (net): Sequential(\n","          (0): Linear(in_features=256, out_features=1024, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=1024, out_features=256, bias=True)\n","          (3): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","  (lm_heads): ModuleList(\n","    (0-3): 4 x Linear(in_features=256, out_features=94, bias=True)\n","  )\n",")"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# This cell loads the model again for further trainning \n","\n","model = GPTLanguageModel()\n","checkpoint = torch.load(\"/kaggle/input/multitoken_30000/pytorch/default/1/multitoken_40k_30000.pth\", map_location=DEVICE)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","# Move the model to the desired device\n","model.to(DEVICE)\n","\n","# Recreate the optimizer after moving the model to the desired device\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","# Move optimizer state to the same device as model\n","for state in optimizer.state.values():\n","    for k, v in state.items():\n","        if isinstance(v, torch.Tensor):\n","            state[k] = v.to(DEVICE)\n","\n","# Set the model to training mode\n","model.train()  # Important for training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for iter in range(MAX_ITERS):\n","\n","    if iter % EVAL_INTERVAL == 0 or iter == MAX_ITERS - 1:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    xb, yb = get_batch('train')\n","\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T22:39:54.910884Z","iopub.status.busy":"2024-09-23T22:39:54.910521Z","iopub.status.idle":"2024-09-23T22:39:55.183548Z","shell.execute_reply":"2024-09-23T22:39:55.182386Z","shell.execute_reply.started":"2024-09-23T22:39:54.910858Z"},"trusted":true},"outputs":[{"data":{"text/plain":["GPTLanguageModel(\n","  (token_embedding_table): Embedding(94, 256)\n","  (position_embedding_table): Embedding(256, 256)\n","  (blocks): Sequential(\n","    (0): Block(\n","      (sa): MultiHeadAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x Head(\n","            (key): Linear(in_features=256, out_features=64, bias=False)\n","            (query): Linear(in_features=256, out_features=64, bias=False)\n","            (value): Linear(in_features=256, out_features=64, bias=False)\n","            (dropout): Dropout(p=0.2, inplace=False)\n","          )\n","        )\n","        (proj): Linear(in_features=256, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","      )\n","      (ffwd): FeedFoward(\n","        (net): Sequential(\n","          (0): Linear(in_features=256, out_features=1024, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=1024, out_features=256, bias=True)\n","          (3): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (1): Block(\n","      (sa): MultiHeadAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x Head(\n","            (key): Linear(in_features=256, out_features=64, bias=False)\n","            (query): Linear(in_features=256, out_features=64, bias=False)\n","            (value): Linear(in_features=256, out_features=64, bias=False)\n","            (dropout): Dropout(p=0.2, inplace=False)\n","          )\n","        )\n","        (proj): Linear(in_features=256, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","      )\n","      (ffwd): FeedFoward(\n","        (net): Sequential(\n","          (0): Linear(in_features=256, out_features=1024, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=1024, out_features=256, bias=True)\n","          (3): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (2): Block(\n","      (sa): MultiHeadAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x Head(\n","            (key): Linear(in_features=256, out_features=64, bias=False)\n","            (query): Linear(in_features=256, out_features=64, bias=False)\n","            (value): Linear(in_features=256, out_features=64, bias=False)\n","            (dropout): Dropout(p=0.2, inplace=False)\n","          )\n","        )\n","        (proj): Linear(in_features=256, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","      )\n","      (ffwd): FeedFoward(\n","        (net): Sequential(\n","          (0): Linear(in_features=256, out_features=1024, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=1024, out_features=256, bias=True)\n","          (3): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (3): Block(\n","      (sa): MultiHeadAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x Head(\n","            (key): Linear(in_features=256, out_features=64, bias=False)\n","            (query): Linear(in_features=256, out_features=64, bias=False)\n","            (value): Linear(in_features=256, out_features=64, bias=False)\n","            (dropout): Dropout(p=0.2, inplace=False)\n","          )\n","        )\n","        (proj): Linear(in_features=256, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","      )\n","      (ffwd): FeedFoward(\n","        (net): Sequential(\n","          (0): Linear(in_features=256, out_features=1024, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=1024, out_features=256, bias=True)\n","          (3): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (4): Block(\n","      (sa): MultiHeadAttention(\n","        (heads): ModuleList(\n","          (0-3): 4 x Head(\n","            (key): Linear(in_features=256, out_features=64, bias=False)\n","            (query): Linear(in_features=256, out_features=64, bias=False)\n","            (value): Linear(in_features=256, out_features=64, bias=False)\n","            (dropout): Dropout(p=0.2, inplace=False)\n","          )\n","        )\n","        (proj): Linear(in_features=256, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","      )\n","      (ffwd): FeedFoward(\n","        (net): Sequential(\n","          (0): Linear(in_features=256, out_features=1024, bias=True)\n","          (1): ReLU()\n","          (2): Linear(in_features=1024, out_features=256, bias=True)\n","          (3): Dropout(p=0.2, inplace=False)\n","        )\n","      )\n","      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","  (lm_heads): ModuleList(\n","    (0-3): 4 x Linear(in_features=256, out_features=94, bias=True)\n","  )\n",")"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["# Load the model for inference\n","model = GPTLanguageModel()\n","optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE)\n","\n","# Load the saved model and optimizer\n","checkpoint = torch.load(\"/kaggle/input/multitoken-50000/multitoken_40k_50000.pth\")\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","# Move the model to the desired device\n","model.to(DEVICE)\n","\n","# Set the model to eval mode\n","model.eval()  "]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-09-23T22:39:56.662948Z","iopub.status.busy":"2024-09-23T22:39:56.661945Z","iopub.status.idle":"2024-09-23T22:40:06.969412Z","shell.execute_reply":"2024-09-23T22:40:06.968255Z","shell.execute_reply.started":"2024-09-23T22:39:56.662914Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","the impacts had comn from Loken.\n","‘You probably have a man who has to get down,’ ordered Tarvitz. ‘It will\n","do it, but must be the primarch’s cities are so not good. He sees brothers,\n","and con you do not know a thing because you will not make three devoted\n","numbers?’\n","‘Don’t be the long time,’ said Tarvitz, ‘and some stronger than my stands. I\n","relish you for the system now. Still, you won’t understand.’\n","‘Then I do not seem convursed,’ said Tarvitz. ‘Whatever you is,’\n","‘You will confirm.’\n","‘You are normal, Loken?’\n","‘For a moment,’ smiled Loken. ‘We can’t have heard that once.’\n","He didn’t open his vox and the thruster caused him with his symbol.\n","‘A world of Since two warriors.’\n","‘Yes,’ looked Tarvitz. ‘Who was the enternal communication to the annex,\n","pushing back at all fours?’\n","‘They can’t be told I have any more point if we can get the amplifier\n","fighting troigers back on this station.’\n","‘I don’t know.’\n","‘I think Tarvitz was found,’ replied Lord Commander Voke. ‘The Fabricator\n","General Caturix moved \n"]}],"source":["# generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device = DEVICE)\n","print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))\n","#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5539335,"sourceId":9167353,"sourceType":"datasetVersion"},{"datasetId":5755681,"sourceId":9465964,"sourceType":"datasetVersion"},{"modelId":102274,"modelInstanceId":77646,"sourceId":92598,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":124091,"modelInstanceId":99918,"sourceId":118816,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
